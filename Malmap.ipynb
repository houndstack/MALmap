{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import community # pip install python-louvain\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import collections\n",
    "import cPickle as pickle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Anime:\n",
    "    def __init__(self, id_num):\n",
    "        self.id_num = id_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial settings\n",
    "threshold = 2;\n",
    "queue = []\n",
    "edge_list = []\n",
    "anime_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tales of Phantasia The Animation | 0\n"
     ]
    }
   ],
   "source": [
    "# Get recommendation data, takes ~2-3 hours\n",
    "\n",
    "queue.append(2167) # Add Clannad <3\n",
    "# queue.insert(0, anime_id) # Run if connection cuts out\n",
    "\n",
    "while len(queue) > 0:\n",
    "    anime_id = queue.pop(0)\n",
    "    anime = Anime(anime_id)\n",
    "    \n",
    "    # Retrieve website data\n",
    "    url = \"https://myanimelist.net/anime/\" + str(anime_id)\n",
    "    rec_url = \"https://myanimelist.net/anime/\" + str(anime_id) +\"/_/userrecs\"\n",
    "    try:\n",
    "        mal_file = urllib2.urlopen(url)\n",
    "        mal_html = mal_file.read()\n",
    "        mal_file.close()\n",
    "        \n",
    "        rec_mal_file = urllib2.urlopen(rec_url)\n",
    "        rec_mal_html = rec_mal_file.read()\n",
    "        rec_mal_file.close()  \n",
    "    except:\n",
    "        time.sleep(60)\n",
    "        print \"Retrying...\"\n",
    "        \n",
    "        mal_file = urllib2.urlopen(url)\n",
    "        mal_html = mal_file.read()\n",
    "        mal_file.close()\n",
    "        \n",
    "        rec_mal_file = urllib2.urlopen(rec_url)\n",
    "        rec_mal_html = rec_mal_file.read()\n",
    "        rec_mal_file.close() \n",
    "        \n",
    "    soup = BeautifulSoup(mal_html, 'lxml')\n",
    "    rec_soup = BeautifulSoup(rec_mal_html, 'lxml')\n",
    "    # print soup.prettify()\n",
    "\n",
    "    # Get anime name\n",
    "    name = soup.find('span', itemprop='name').get_text()\n",
    "    anime.name = name\n",
    "\n",
    "    # Get synopsis\n",
    "    synopsis = soup.find('span', itemprop='description').get_text()\n",
    "    anime.synopsis = synopsis\n",
    "\n",
    "    # Get score\n",
    "    score_text = soup.find('div', \"score\").get_text()\n",
    "    score = float(score_text)\n",
    "    anime.score = score\n",
    "\n",
    "    # Get rank\n",
    "    rank_text = soup.find('span', \"numbers ranked\").get_text()\n",
    "    rank_num = re.findall('\\d+', rank_text)\n",
    "    if len(rank_num) > 0:\n",
    "        rank = int(rank_num[0])\n",
    "    else:\n",
    "        rank = 0\n",
    "    anime.rank = rank\n",
    "\n",
    "    # Get popularity\n",
    "    pop_text = soup.find('span', \"numbers popularity\").get_text()\n",
    "    pop_num = re.findall('\\d+', pop_text)\n",
    "    if len(pop_num) > 0:\n",
    "        popularity = int(pop_num[0])\n",
    "    else:\n",
    "        popularity = 0\n",
    "    anime.popularity = popularity\n",
    "\n",
    "    # Get members\n",
    "    mem_text = soup.find('span', \"numbers members\").get_text()\n",
    "    mem_text = mem_text.replace(',', '')\n",
    "    mem_num = re.findall('\\d+', mem_text)\n",
    "    if len(mem_num) > 0:\n",
    "        members = int(mem_num[0])\n",
    "    else:\n",
    "        members = 0\n",
    "    anime.members = members\n",
    "\n",
    "    # Get episodes\n",
    "    stats = soup.find_all('div', \"spaceit\")\n",
    "    for s in stats:\n",
    "        stat = s.get_text()\n",
    "        if \"Episodes:\" in stat:\n",
    "            eps_num = re.findall('\\d+', stat)\n",
    "            if len(eps_num) > 0:\n",
    "                episodes = int(eps_num[0])\n",
    "            else:\n",
    "                episodes = 0\n",
    "    anime.episodes = episodes\n",
    "\n",
    "    # Get season\n",
    "    season_name = soup.select('a[href^=\"https://myanimelist.net/anime/season/\"]')\n",
    "    if len(season_name) > 0:\n",
    "        season = season_name[0].get_text()\n",
    "    else:\n",
    "        season = ''\n",
    "    anime.season = season\n",
    "\n",
    "    # Get genres\n",
    "    genre_list = [];\n",
    "    genre_names = [];\n",
    "    genres = soup.select('a[href^=\"/anime/genre/\"]')\n",
    "    for g in genres:\n",
    "        genre_text = g.get('href')\n",
    "        genre = re.findall('\\d+', genre_text)\n",
    "        genre_list.append(int(genre[0]))\n",
    "        genre_names.append(g.get_text())\n",
    "    anime.genre_names = genre_names\n",
    "    anime.genre_list = genre_list\n",
    "\n",
    "    # Download image\n",
    "    img_url= soup.find('img', itemprop='image').get('src')\n",
    "    # urllib.urlretrieve(str(img_url), 'images/' + str(anime_id) + '.jpg')\n",
    "    anime.img_url = img_url\n",
    "\n",
    "    # Get recommendation numbers\n",
    "    rec_tags = rec_soup.find_all('a', \"js-similar-recommendations-button\")\n",
    "    rec_list = [];\n",
    "    for rec_tag_strs in rec_tags:\n",
    "        rec_text = rec_tag_strs.get_text()\n",
    "        parsed_rec = re.findall('\\d+', rec_text)\n",
    "        num_recs = int(parsed_rec[0])\n",
    "        if num_recs >= threshold:\n",
    "            rec_list.append(num_recs)\n",
    "    anime.rec_list = rec_list\n",
    "\n",
    "    # Get recommendation ids\n",
    "    recs = rec_soup.find_all('a', \"hoverinfo_trigger\")\n",
    "    id_list = []\n",
    "    for rec_names in recs:\n",
    "        rec_name = rec_names.get('href')\n",
    "        rec_anime_id = re.findall('\\d+', rec_name)\n",
    "        id_list.append(int(rec_anime_id[0]))\n",
    "    id_list = id_list[:len(rec_list)]\n",
    "    anime.id_list = id_list\n",
    "    \n",
    "    # Add anime to visited dictionary, new anime to queue, new edges to edge list\n",
    "    anime_list[anime_id] = anime\n",
    "    for index, id_num in enumerate(id_list):\n",
    "        if not anime_list.has_key(id_num):\n",
    "            if id_num not in queue:\n",
    "                queue.append(id_num)\n",
    "            edge_list.append([anime_id, id_num, rec_list[index]])\n",
    "    \n",
    "    clear_output()\n",
    "    print anime.name + ' | ' + str(len(queue))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Post-processing and data storage\n",
    "\n",
    "# Generate communities\n",
    "G = nx.Graph()\n",
    "for k in anime_list:\n",
    "    G.add_node(k)\n",
    "for e in edge_list:\n",
    "    G.add_edge(e[0],e[1],weight=e[2])\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "# Add communities to anime data\n",
    "count = 0\n",
    "for com in set(partition.values()) :\n",
    "    count = count + 1\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    for n in list_nodes:\n",
    "        a = anime_list[n]\n",
    "        a.community = count\n",
    "\n",
    "# Generate .csv files for import into Gephi\n",
    "# Export edges\n",
    "with open('edges.csv', 'wb') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow([\"Source\", \"Target\", \"Weight\", \"Type\"])\n",
    "    for e in edge_list:\n",
    "        line = list(e)\n",
    "        line.append(\"Undirected\")\n",
    "        writer.writerow(line)\n",
    "\n",
    "# Export nodes\n",
    "with open('nodes.csv', 'wb') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow([\"ID\", \"Label\", \"Image\", \"Community\"])\n",
    "    for cur_id, a in anime_list.items():\n",
    "        writer.writerow([cur_id, a.name.encode('utf-8'), a.img_url, a.community])\n",
    "\n",
    "# Save anime list data\n",
    "pickle.dump(anime_list, open('save.p', 'wb'))\n",
    "  \n",
    "# Draw network\n",
    "# pos = nx.spring_layout(G)\n",
    "# size = float(len(set(partition.values())))\n",
    "# count = 0\n",
    "# for com in set(partition.values()) :\n",
    "#     count = count + 1\n",
    "#     list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "#     for n in list_nodes:\n",
    "#         nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20, node_color = str(count / size))\n",
    "# nx.draw_networkx_edges(G,pos, alpha=0.5)\n",
    "# plt.show()\n",
    "        \n",
    "# Export all node data       \n",
    "# with open('nodes.csv', 'wb') as csv_file:\n",
    "#     writer = csv.writer(csv_file, delimiter=',')\n",
    "#     writer.writerow([\"ID\", \"Label\", \"Score\", \"Rank\", \"Popularity\", \"Members\", \n",
    "#                      \"Episodes\", \"Season\", \"Genres\", \"Genre IDS\", \"MAL Link\", \n",
    "#                      \"Image\", \"Synopsis\", \"Community\"])\n",
    "#     for cur_id, a in anime_list.items():\n",
    "#         genre_str = '';\n",
    "#         genre_id_str = '';\n",
    "#         for g in a.genre_names:\n",
    "#             if genre_str is not '':\n",
    "#                 genre_str = genre_str + ', '\n",
    "#             genre_str = genre_str + g\n",
    "#         for g_id in a.genre_list:\n",
    "#             if genre_id_str is not '':\n",
    "#                 genre_id_str = genre_id_str + ', '\n",
    "#             genre_id_str = genre_id_str + str(g_id)\n",
    "#         cur_url = '<a href=\"https://myanimelist.net/anime/' + str(cur_id) + '\">'+ a.name.encode('utf-8') + '</a>'\n",
    "#         cur_img_url = '<img src=\"' + a.img_url + '\" alt=\"' + a.name.encode('utf-8') + '\">'\n",
    "#         writer.writerow([cur_id, a.name.encode('utf-8'), a.score, \"#\"+str(a.rank), \"#\"+str(a.popularity), \n",
    "#                          '{:,}'.format(a.members), a.episodes, a.season, genre_str, genre_id_str, cur_url, \n",
    "#                          a.img_url, a.synopsis.encode('utf-8'), a.community])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instructions for Gephi (for consistency)\n",
    "# 1. Import nodes, import edges\n",
    "# 2. Run OpenOrd layout, then run ForceAtlas2 until stable\n",
    "# 3. Rotate until at a desirable position\n",
    "# 4. Change node color to be based on community, use the Fluorescent palette\n",
    "# 5. Change node size to be based on degree, with min = 1, max = 5, and a circular distribution\n",
    "# 6. Change node text size to be based on degree, with min = 1, max = 5, and a circular distribution\n",
    "# 7. Slide text size to the smallest font, set font size to 14, then run LabelAdjust\n",
    "# 8. Modify any of the above parameters for a more preferred layout\n",
    "# 9. Export to sigma.js template, uncheck the replace node ids with numbers box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Export generic network graph showing detected communities\n",
    "\n",
    "# Import anime_list object\n",
    "anime_list = pickle.load(open('save.p', 'rb'))\n",
    "\n",
    "# Load exported data file from Gephi\n",
    "with open('network/data.json') as data_file:    \n",
    "    sigma_data = json.load(data_file)\n",
    "    data_file.close()\n",
    "\n",
    "# Modify attributes\n",
    "for d in sigma_data['nodes']:\n",
    "    cur_id = int(d['id'])\n",
    "    a = anime_list.get(cur_id)\n",
    "    attr = collections.OrderedDict()\n",
    "    attr['Score'] = a.score\n",
    "    attr['Rank'] = '#' + str(a.rank)\n",
    "    attr['Popularity'] = '#' + str(a.popularity)\n",
    "    attr['Members'] = '{:,}'.format(a.members)\n",
    "    attr['Episodes'] = a.episodes\n",
    "    attr['Season'] = a.season\n",
    "    genre_str = '';\n",
    "    for g in a.genre_names:\n",
    "        if genre_str is not '':\n",
    "            genre_str = genre_str + ', '\n",
    "        genre_str = genre_str + g\n",
    "    attr['Genres'] = genre_str\n",
    "    cur_url = '<a href=\"https://myanimelist.net/anime/' + str(cur_id) + '\">' + a.name.encode('utf-8') + '</a>'\n",
    "    attr['MAL Link'] = cur_url\n",
    "    attr['Synopsis'] = a.synopsis.encode('utf-8')\n",
    "    attr['Image'] = a.img_url\n",
    "    d['attributes'] = attr\n",
    "    \n",
    "for d in sigma_data['edges']:\n",
    "    d['color'] = 'rgb(150,150,150)'\n",
    "\n",
    "# Export new data file\n",
    "with open('MALmap/data.json', 'w') as outfile:  \n",
    "    json.dump(sigma_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: igfod13\n"
     ]
    }
   ],
   "source": [
    "# Export user's MAL network graph\n",
    "\n",
    "# Import anime_list object\n",
    "anime_list = pickle.load(open('save.p', 'rb'))\n",
    "\n",
    "# username = \"igfod13\"\n",
    "username = raw_input(\"Username: \")\n",
    "\n",
    "# Load user's anime list\n",
    "url = \"https://myanimelist.net/malappinfo.php?status=all&type=anime&u=\" + username\n",
    "response = requests.get(url)\n",
    "# print (response.status_code)\n",
    "tree = ET.fromstring(response.content)\n",
    "\n",
    "# Retrieve user data\n",
    "user_list = {}\n",
    "user_score_list ={}\n",
    "for anime in tree:\n",
    "    for item in anime:\n",
    "        if item.tag == 'series_animedb_id':\n",
    "            temp_id = int(item.text)\n",
    "        if item.tag == 'my_status':\n",
    "            temp_status = int(item.text)\n",
    "        if item.tag == 'my_score':\n",
    "            temp_score = int(item.text)\n",
    "    if anime.tag == 'anime':\n",
    "        user_list[temp_id] = temp_status\n",
    "        user_score_list[temp_id] = temp_score\n",
    "\n",
    "# Get user's score distribution\n",
    "user_data = []    \n",
    "for i, a in user_score_list.items():\n",
    "    if a != 0:\n",
    "        user_data.append(a)\n",
    "a = np.array(user_data)\n",
    "user_percentile = np.percentile(a, 65)\n",
    "\n",
    "# Get total score distribution\n",
    "data = []\n",
    "for cur_id, a in anime_list.items():\n",
    "    data.append(a.score)\n",
    "a = np.array(data)\n",
    "p1 = np.percentile(a, 90)\n",
    "p2 = np.percentile(a, 70)\n",
    "p3 = np.percentile(a, 40) \n",
    "\n",
    "# MAL status IDS: 1 = watching, 2 = completed, 3 = on hold, 4 = dropped, 6 = plan to watch\n",
    "# Additional IDS: 9 = highly ranked by user, 10-13 = top 0-10/10-30/30-60/60-100% ranked unwatched\n",
    "for cur_id, a in anime_list.items():\n",
    "    if user_list.has_key(cur_id):\n",
    "        if user_list[cur_id] == 2 and user_score_list[cur_id] > user_percentile:\n",
    "                a.user_status = 9\n",
    "        else:\n",
    "            a.user_status = user_list[cur_id]\n",
    "    else:\n",
    "        if a.score > p1:\n",
    "            a.user_status = 10\n",
    "        elif a.score > p2:\n",
    "            a.user_status = 11\n",
    "        elif a.score > p3:\n",
    "            a.user_status = 12\n",
    "        else:\n",
    "            a.user_status = 13\n",
    "            \n",
    "# Load exported data file from Gephi\n",
    "# with open('network/data.json') as data_file:\n",
    "with open('data.json') as data_file:  \n",
    "    sigma_data = json.load(data_file)\n",
    "    data_file.close()\n",
    "\n",
    "# Modify attributes\n",
    "for d in sigma_data['nodes']:\n",
    "    cur_id = int(d['id'])\n",
    "    a = anime_list.get(cur_id)\n",
    "    attr = collections.OrderedDict()\n",
    "    attr['Score'] = a.score\n",
    "    attr['Rank'] = '#' + str(a.rank)\n",
    "    attr['Popularity'] = '#' + str(a.popularity)\n",
    "    attr['Members'] = '{:,}'.format(a.members)\n",
    "    attr['Episodes'] = a.episodes\n",
    "    attr['Season'] = a.season\n",
    "    genre_str = '';\n",
    "    for g in a.genre_names:\n",
    "        if genre_str is not '':\n",
    "            genre_str = genre_str + ', '\n",
    "        genre_str = genre_str + g\n",
    "    attr['Genres'] = genre_str\n",
    "    cur_url = '<a href=\"https://myanimelist.net/anime/' + str(cur_id) + '\">' + a.name.encode('utf-8') + '</a>'\n",
    "    attr['MAL Link'] = cur_url\n",
    "    attr['Synopsis'] = a.synopsis.encode('utf-8')\n",
    "    attr['Image'] = a.img_url\n",
    "    d['attributes'] = attr\n",
    "    # Set node colors\n",
    "    if a.user_status == 1:\n",
    "        d['color'] = 'rgb(45,175,55)'\n",
    "    elif a.user_status == 2:\n",
    "        d['color'] = 'rgb(40,80,210)'\n",
    "    elif a.user_status == 3:\n",
    "        d['color'] = 'rgb(250,210,85)'\n",
    "    elif a.user_status == 4:\n",
    "        d['color'] = 'rgb(160,45,50)'\n",
    "    elif a.user_status == 6:\n",
    "        d['color'] = 'rgb(255,155,45)'\n",
    "    elif a.user_status == 9:\n",
    "        d['color'] = 'rgb(50,190,255)'\n",
    "    elif a.user_status == 10:\n",
    "        d['color'] = 'rgb(195,195,195)'\n",
    "    elif a.user_status == 11:\n",
    "        d['color'] = 'rgb(155,155,155)'\n",
    "    elif a.user_status == 12:\n",
    "        d['color'] = 'rgb(115,115,115)'\n",
    "    else:\n",
    "        d['color'] = 'rgb(75,75,75)'\n",
    "    \n",
    "for d in sigma_data['edges']:\n",
    "    d['color'] = 'rgb(75,75,75)'\n",
    "\n",
    "# Export new data file\n",
    "# with open('MALmap/data.json', 'w') as outfile:  \n",
    "with open('data.json', 'w') as outfile:  \n",
    "    json.dump(sigma_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
